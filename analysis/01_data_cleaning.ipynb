{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary\n",
    "\n",
    "This notebook will explain how the data cleaning is done in the model. Model data cleaning code can be found in the `model/data_processing` directory.\n",
    "\n",
    "### Drop N/A Values from these columns\n",
    "- delinq_2yrs\n",
    "- days_since_earliest_cr_line\n",
    "- inq_last_6mths\n",
    "- open_acc\n",
    "- pub_rec\n",
    "- total_acc\n",
    "- revol_util\n",
    "- purpose\n",
    "- annual_inc\n",
    "- emp_length\n",
    "\n",
    "\n",
    "### Drop Columns\n",
    "- collections_12_mths_ex_med\n",
    "    - All values are either 0 or N/A\n",
    "- pymnt_plan, \n",
    "- initial_list_status\n",
    "    - Huge imbalances, will likely lead to overfitting. Double check.\n",
    "- mths_since_last_record\n",
    "    - Large number of N/A values, not immediately clear how to impute. Noted for a later version of the model.\n",
    "- mths_since_last_delinq\n",
    "    - Large number of N/A values, not immediately clear how to impute. Noted for a later version of the model.\n",
    "- zip_code\n",
    "    - Not informative in model testing, removing for V1.\n",
    "- addr_state\n",
    "    - The data in this column is just to sparse for V1.\n",
    "- emp_title\n",
    "    - Will not make it into V1.\n",
    "\n",
    "    \n",
    "### Drop Outliers/Nonsensical Data\n",
    "- revol_bal == 1207359, this is an outlier value.\n",
    "- revol_util > 100\n",
    "\n",
    "### Simplify/Cleanup Categorical Data\n",
    "- emp_title\n",
    "    - This can be dramatically cleaned up, but is going to be dropped from V1\n",
    "- home_ownership, verification_status\n",
    "    - Simplify\n",
    "\n",
    "### Distill Commentary (extra)\n",
    "- Notes\n",
    "    - Will likely drop for V1.\n",
    "- purpose\n",
    "    - Will likely drop and use purpose_cat instead.\n",
    "- purpose_cat\n",
    "    - Will choose cutoff count (<100) and map all those values to an 'other' bucket.\n",
    "\n",
    "\n",
    "## Final Transforms\n",
    "- Dummies will be created for categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T21:36:00.499902Z",
     "start_time": "2019-04-29T21:35:59.809043Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('../data/DR_Demo_Lending_Club.csv')\n",
    "orig_df_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.808Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create new variable from earliest_cr_line(datetime) = \n",
    "## current_date - earliest_cr_line = days since earliest credit line\n",
    "current_date = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "df['days_since_earliest_cr_line'] = (current_date - df['earliest_cr_line']).dt.days\n",
    "df = df.drop('earliest_cr_line', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.811Z"
    }
   },
   "outputs": [],
   "source": [
    "## Convert emp_length to float64 from str.\n",
    "df.loc[df['emp_length'] == 'na', 'emp_length'] = np.nan\n",
    "df['emp_length'] = df['emp_length'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.816Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop NA Values from the following columns\n",
    "drop_na_value_columns = ['delinq_2yrs',\n",
    "'days_since_earliest_cr_line',\n",
    "'inq_last_6mths',\n",
    "'open_acc',\n",
    "'pub_rec',\n",
    "'total_acc',\n",
    "'revol_util',\n",
    "'purpose',\n",
    "'annual_inc']\n",
    "\n",
    "df = df.dropna(axis=0, subset=drop_na_value_columns)\n",
    "print('Rows dropped: {}'.format(orig_df_length - len(df)))\n",
    "print('New data length: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.820Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping the following for V1 of the model\n",
    "df = df.drop(['collections_12_mths_ex_med', \n",
    "             'pymnt_plan', \n",
    "             'initial_list_status',\n",
    "             'mths_since_last_record',\n",
    "             'mths_since_last_delinq',\n",
    "             'zip_code',\n",
    "             'addr_state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.824Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping very specific outliers/non-sensical rows\n",
    "df = df[df['revol_util'] <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.835Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating mapping to consolidate 'VERIFIED - income' and 'VERIFIED - income source' as simply ''VERIFIED - income'\n",
    "value_map = {\n",
    "    'VERIFIED - income': 'VERIFIED - income',\n",
    "    'VERIFIED - income source': 'VERIFIED - income',\n",
    "    'not verified': 'not verified'\n",
    "}\n",
    "df['verification_status'] = [value_map[x] for x in df['verification_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.842Z"
    }
   },
   "outputs": [],
   "source": [
    "## Mapping lower represented groups to an 'other' bucket\n",
    "purpose_cat_count = df.groupby(['purpose_cat']).count().sort_values('Id', ascending=False)['Id']\n",
    "valid_values = list(purpose_cat_count[purpose_cat_count > 100].index)\n",
    "df['purpose_cat'] = [purpose if purpose in valid_values else 'other' for purpose in df['purpose_cat']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.846Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.850Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping following text columns for V1\n",
    "df = df.drop(['emp_title', 'Notes','purpose'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.865Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting out categorical data to dummies:\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "categorical_columns = df.select_dtypes(exclude=numerics)\n",
    "\n",
    "for col in categorical_columns:\n",
    "    dummies = pd.get_dummies(categorical_columns[col])\n",
    "    df = df.drop(col, axis=1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.870Z"
    }
   },
   "outputs": [],
   "source": [
    "## Final df:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.873Z"
    }
   },
   "outputs": [],
   "source": [
    "from library_code import clean_and_transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.875Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/DR_Demo_Lending_Club.csv')\n",
    "# df = clean_and_transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T21:35:59.878Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DR_TH",
   "language": "python",
   "name": "py2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
